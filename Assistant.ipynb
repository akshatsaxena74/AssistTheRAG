{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7fsqQWucx-B",
    "outputId": "c3cbb4c0-f85e-4e5e-c963-fa321a65f296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m2.3/2.4 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install huggingface_hub langchain langchain_community huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWQ2MnjEFj7F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"...\"\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z1I2xzc_Pkiv"
   },
   "outputs": [],
   "source": [
    "summarizer_prompt_template = \"\"\"\n",
    "You are a helpful AI assistant who is provided with a query, context and some previous chats with the user.\n",
    "You have to predict whether the context and the previous memory retrieved by the retriever will be helpful while answering the giving query.\n",
    "Craft a summarised version of the entire context and memory, focusing only the important information that you believe might prove useful while \n",
    "answering the user's query.\n",
    "\n",
    "The query is:\n",
    "{question}\n",
    "\n",
    "Retrieved Context : {context}\n",
    "Previous chats : {prev_chats}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Generate 2 search queries that can be used to find information relevant to the given question.\n",
    "Question: {question}.\n",
    "Return the output in JSON format\n",
    "Search queries: \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lM4UIjA6Mgr4"
   },
   "outputs": [],
   "source": [
    "class pipeline():\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  def predict(self, prompt):\n",
    "    llm = HuggingFaceHub(\n",
    "    repo_id=self.model,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 1.0,\n",
    "        # \"max_length\": self.max_length,\n",
    "        \"max_new_tokens\": 512,\n",
    "    }\n",
    "    )\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "class assistant_model(pipeline):\n",
    "  def __init__(self, model, prompt_template, summarizer_prompt_template):\n",
    "    super().__init__(model,)\n",
    "    self.prompt_template = prompt_template\n",
    "    self.summarizer_prompt_template = summarizer_prompt_template\n",
    "\n",
    "  def QA_decomposition(self, query):\n",
    "      formatted_query = self.prompt_template.format(question=query)\n",
    "      raw_answer = self.predict(formatted_query)\n",
    "      match = re.search(r\"({[^}]*})\", raw_answer, re.DOTALL)\n",
    "      if match:\n",
    "          json_output = match.group(1).strip()\n",
    "          result = json_output\n",
    "      else:\n",
    "          result = raw_answer.strip()\n",
    "      result = json.loads(result)\n",
    "      result[\"query3\"] = query\n",
    "      \n",
    "      return result\n",
    "        \n",
    "  def summarizer(self, query, context, prev_chats):\n",
    "    formatted_query = self.summarizer_prompt_template.format(question=query, context = context, prev_chats = prev_chats)\n",
    "    raw_answer = self.predict(formatted_query)\n",
    "    match = re.search(r\"(1\\..+)\\n(2\\..+)\", raw_answer, re.DOTALL)\n",
    "    if match:\n",
    "        query1 = match.group(1).strip()\n",
    "        query2 = match.group(2).strip()\n",
    "        return f\"{query}\\n{query1}\\n{query2}\"\n",
    "    else:\n",
    "        return raw_answer.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqYRwzRzRSv8",
    "outputId": "07aa4035-8db3-49de-a15a-acaed88998b4"
   },
   "outputs": [],
   "source": [
    "# model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "query = \"Who won the football word cup 2022\"\n",
    "\n",
    "assistant = assistant_model(model, prompt_template, summarizer_prompt_template, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lR0doxY7TVP8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query1': '2022 football world cup winner', 'query2': '2022 FIFA world cup champion', 'query3': 'Who won the football word cup 2022'}\n"
     ]
    }
   ],
   "source": [
    "result = assistant.QA_decomposition(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to Colaboratory",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

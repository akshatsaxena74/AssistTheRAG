{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7fsqQWucx-B",
    "outputId": "c3cbb4c0-f85e-4e5e-c963-fa321a65f296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m2.3/2.4 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install huggingface_hub langchain langchain_community sentence-transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ZWQ2MnjEFj7F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"...\"\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "BdZOdY5sMdY2"
   },
   "outputs": [],
   "source": [
    "class pipeline():\n",
    "  def __init__(self, model, temperature, max_new_tokens, max_length):\n",
    "    self.model = model\n",
    "    self.temperature = temperature\n",
    "    self.max_new_tokens = max_new_tokens\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def predict(self, prompt):\n",
    "    llm = HuggingFaceHub(\n",
    "    repo_id=self.model,\n",
    "    model_kwargs={\n",
    "        \"temperature\": self.temperature,\n",
    "        \"max_length\": self.max_length,\n",
    "        \"max_new_tokens\": self.max_new_tokens,\n",
    "    }\n",
    "    )\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Z1I2xzc_Pkiv"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful AI assistant who provides 2 queries that mean essentially the same as the asked query.\n",
    "Do not try to answer the query.\n",
    "Do the aforementioned task for the given query:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "lM4UIjA6Mgr4"
   },
   "outputs": [],
   "source": [
    "class assistant_model(pipeline):\n",
    "  def __init__(self, model, prompt_template, query, temperature, max_new_tokens, max_length):\n",
    "    super().__init__(model, temperature, max_new_tokens, max_length)\n",
    "    self.prompt_template = prompt_template\n",
    "    self.query = query\n",
    "\n",
    "  def QA_decomposition(self):\n",
    "      formatted_query = self.prompt_template.format(question=self.query)\n",
    "      raw_answer = self.predict(formatted_query)\n",
    "      match = re.search(r\"(1\\..+)\\n(2\\..+)\", raw_answer, re.DOTALL)\n",
    "      if match:\n",
    "          query1 = match.group(1).strip()\n",
    "          query2 = match.group(2).strip()\n",
    "          return f\"{self.query}\\n{query1}\\n{query2}\"\n",
    "      else:\n",
    "          return raw_answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqYRwzRzRSv8",
    "outputId": "07aa4035-8db3-49de-a15a-acaed88998b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who won the football world cup 2024?\n",
      "1. What was the winner of the FIFA World Cup competition in the year 2024?\n",
      "2. Who is the champion of the FIFA World Cup that took place in the year 2024?\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "query = \"Who won the football world cup 2024?\"\n",
    "temperature = 0.7\n",
    "max_new_tokens = 500\n",
    "max_length = 200\n",
    "\n",
    "assistant = assistant_model(model, prompt_template, query, temperature, max_new_tokens, max_length)\n",
    "\n",
    "result = assistant.QA_decomposition()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR0doxY7TVP8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to Colaboratory",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
